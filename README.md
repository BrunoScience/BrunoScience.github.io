# 100 Days of learning about graph compute and *ab initio* quantum chemistry methods

In some sense, this deep dive is about the particular topic of [*ab initio* quantum chemistry](https://en.wikipedia.org/wiki/Ab_initio_quantum_chemistry_methods) and how that background might help us do [computational chemistry methods](https://en.wikipedia.org/wiki/Computational_chemistry) or, perhaps, how we might optimize [**Physics-informed** neural networks (PINN)](https://docs.nvidia.com/deeplearning/modulus/user_guide/theory/phys_informed.html) to be more efficient in searching for or identifying compounds with properties we desire.

## Learning is about graphs, about REALLY *connecting* the dots

We want the executable compute graph of something, we want to know how to put it into practice ... this means going beyond simply learning what word means or how something works ... we want our simulations to be meaningful and accurate ... we want the digital twin of something to approach being a digital clone.

This mean that our understanding must be operational, functional, executable ... so we must learn ***the GRAPH*** of a topic ... and since we use machines, it's about learning the ***compute graph*** of a topic.  To achieve that, we will need to learn the human knowledge graph, the commitgraph of who's made what commit to the body of knowledge and what impact that commit has had ... and we also want to understand something about where the body of knowledge seems to be going, so we will want to understand the social graph of a topic, especially if we are contrarians or skeptics ... LEARNING is about how people, knowledge, topics, things relate to one another **and where the gaps are** ... ***LEARNING is about really CONNECTING the dots AND understanding where the connections are imprecise or shabby.***


Learning about learning is about learning how to level up skills as we go along ... not just how to test, experiment and learn -- but how the best practices in testing, experimentation and learning are improving in near realtime as we learn them.  We want to develop the skills of learning faster about things that can make one more productive. Active learning of this nature is fundamentally an exercise in the continuous improvement of diligent dogfooding -- we are mostly learning in order to learn in order to learn in order to learn ...  we use deep learning and computational horsepower to aid our learning about other things like computational chemistry and simulation, AI for computer-aided imaging at nanoscale and natural language processing for smarter ways to curate and annotate datagraphs of knowledge.

In the end ... it's mostly about the ***discipline*** of continuously learning about learning.
